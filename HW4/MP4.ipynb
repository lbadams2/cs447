{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MP4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gau9xEXMGY8s",
        "colab_type": "text"
      },
      "source": [
        "# Using Attention for Neural Machine Translation\n",
        "In this notebook we are going to perform machine translation using a deep learning based approach and attention mechanism.\n",
        "\n",
        "Specifically, we are going to train a sequence to sequence model for Spanish to English translation.  We will use Sequence to Sequence Models for this Assignment. In this assignment you only need tto implement the encoder and decoder, we implement all the data loading for you.Please **refer** to the following resources for more details:\n",
        "\n",
        "1.   https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
        "2.   https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "3. https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9mf5x3zHp1A",
        "colab_type": "code",
        "outputId": "764a9ccb-a1d2-4b90-c3de-05bd4877c734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.0+cu100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XH8nu0ojQpt",
        "colab_type": "text"
      },
      "source": [
        "# Download The Data\n",
        "\n",
        "Here we will download the translation data. We will learn a model to translate Spanish to English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftasb3wEH0gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j41KgqCVIIjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd sample_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtyBFlMKIg7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGnC0q_SI5qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC4Pg0kgLxqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('spa.txt', encoding='UTF-8').read().strip().split('\\n')\n",
        "lines = f\n",
        "total_num_examples = 30000 \n",
        "original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines[:total_num_examples]]\n",
        "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"es\"])\n",
        "data # visualizing the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_WR8vEGMQyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Preprocessing the sentence to add the start, end tokens and make them lower-case\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mud7HbQUMUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "# Make sure YOU only run this once - if you run it twice it will mess up the data so you will have run the few above cells again\n",
        "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"es\"] = data.es.apply(lambda w: preprocess_sentence(w))\n",
        "data[250:260]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHJw_CyykmMp",
        "colab_type": "text"
      },
      "source": [
        "# Vocabulary Class\n",
        "\n",
        "We create a class here for managing our vocabulary as we did in MP2. In this MP, we have a separate class for the vocabulary as we need 2 different vocabularies - one for English and one for Spanish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h4Q21azMW-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab_Lang():\n",
        "    def __init__(self, data):\n",
        "        \"\"\" data is the list of all sentences in the language dataset\"\"\"\n",
        "        self.data = data\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for sentence in self.data:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(sentence.split(' '))\n",
        "\n",
        "        # add a padding token\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWiv0o-8MmXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# index language using the class above\n",
        "inp_lang = Vocab_Lang(data[\"es\"].values.tolist())\n",
        "targ_lang = Vocab_Lang(data[\"eng\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in es.split(' ')]  for es in data[\"es\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bsMZzw4MqJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCw4JM-Mrud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the max_length of input and output tensor for padding\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SRtlYCdMtSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: padded[:] = x[:max_len]\n",
        "    else: padded[:len(x)] = x\n",
        "    return padded\n",
        "\n",
        "# pad all the sentences in the dataset with the max_length\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5YR39L9MwLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training and test/val sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = input_tensor[:24000], input_tensor[24000:], target_tensor[:24000], target_tensor[24000:]\n",
        "\n",
        "assert(len(input_tensor_train)==24000)\n",
        "assert(len(target_tensor_train)==24000)\n",
        "assert(len(input_tensor_val)==6000)\n",
        "assert(len(target_tensor_val)==6000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a2c34aFnPOP",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader for our Encoder and Decoder\n",
        "\n",
        "We prepare the dataloader and make sure the dataloader returns the source sentence, target sentence and the length of the source sentenc sampled from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c797aZAWMzrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwlsuoMSM1uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 60\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)\n",
        "\n",
        "val_dataset = DataLoader(val_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENeT1fj_2f8t",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Model\n",
        "\n",
        "First we build a simple encoder model, which will be very similar to what you did in MP2. But instead of using a fully connected layer as the output, you should the return the output of your recurrent net (GRU/LSTM) as well as the hidden output. They are used in the decoder later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sx4QQd3M4XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Feel free to change any parameters class definitions as long as you can change the training code, but make sure\n",
        "## evaluation should get the tensor format it expects\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        ### TO - DO\n",
        "        \n",
        "    def forward(self, x, lens):\n",
        "        '''\n",
        "        Pseudo-code\n",
        "        - Pass x through an embedding layer\n",
        "        - Make sure x is correctly packed before the recurrent net \n",
        "        - Pass it through the recurrent net\n",
        "        - Make sure the output is unpacked correctly\n",
        "        - return output and hidden states from the recurrent net\n",
        "        - Feel free to play around with dimensions - the training loop should help you determine the dimensions\n",
        "        '''\n",
        "        ### TO - DO\n",
        "        \n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKwsEWpK2mcT",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Model\n",
        "We will implement a Decoder model which uses an attention mechanism. We will implement the decoder as provided in https://arxiv.org/pdf/1409.0473.pdf. **Please read** the links provided above first, at the start of this assignment for review. The pseudo-code for your implementation should be somewhat as follows:\n",
        "\n",
        "\n",
        "\n",
        "1.   The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
        "2.   Using the output your encoder you will calculate the score and subsequently the attention using following equations : \n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "3. Once you have calculated this attention vector, you pass the original input x through a embedding layer. The output of this embedding layer is concatenated with the attention vector which is passed into a GRU.\n",
        "\n",
        "4. Finally you pass the output of the GRU into a fully connected layer with an output size same as that vocab, to see the probability of the most possible word.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw84M2LPM-PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Feel free to change any parameters class definitions as long as you can change the training code, but make sure\n",
        "## evaluation should get the tensor format it expects\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        ### TO - DO\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        '''\n",
        "        Pseudo-code\n",
        "        - Calculate the score using the formula shown above using encoder output and hidden output. \n",
        "        Note h_t is the hidden output of the decoder and h_s is the encoder output in the formula\n",
        "        - Calculate the attention weights using softmax and \n",
        "        passing through V - which can be implemented as a fully connected layer\n",
        "        - Finally find c_t which is a context vector where the shape of context_vector should be (batch_size, hidden_size)\n",
        "        - You need to unsqueeze the context_vector for concatenating with x aas listed in Point 3 above\n",
        "        - Pass this concatenated tensor to the GRU and follow as specified in Point 4 above\n",
        "\n",
        "        Returns :\n",
        "        output - shape = (batch_size, vocab)\n",
        "        hidden state - shape = (batch_size, hidden size)\n",
        "        '''\n",
        "        ### TO - DO\n",
        "        \n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxYamsE3M6u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp2rKJY4NIzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjbsUkcpNK9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "## Feel free to change any parameters class definitions as long as you can change the training code, but make sure\n",
        "## evaluation should get the tensor format it expects, this is only for reference\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaH022cEy03q",
        "colab_type": "text"
      },
      "source": [
        "# Train your model\n",
        "\n",
        "You will train your model here.\n",
        "*   Pass the source sentence and their corresponding lengths into the encoder\n",
        "*   Creating the decoder input using <start> tokens\n",
        "*   Now we find out the decoder outputs conditioned on the previous predicted word usually, but in our training we use teacher forcing. Read more about teacher forcing at https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9663UYJNMgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        # run code below for every timestep in the ys batch\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw9gCFY01lZ2",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "\n",
        "*   We evaluate on the test set.\n",
        "*   In this evaluation, instead of using the concept of teacher forcing, we use the prediction of the decoder as the input to the decoder for the sequence of outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1emvSSo0NRbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "final_output = torch.zeros((len(target_tensor_val),max_length_tar))\n",
        "target_output = torch.zeros((len(target_tensor_val),max_length_tar))\n",
        "\n",
        "for (batch, (inp, targ, inp_len)) in enumerate(val_dataset):\n",
        "    loss = 0\n",
        "    xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "    enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "    dec_hidden = enc_hidden\n",
        "    \n",
        "    dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "    curr_output = torch.zeros((ys.size(0), ys.size(1)))\n",
        "    curr_output[:, 0] = dec_input.squeeze(1)\n",
        "\n",
        "    for t in range(1, ys.size(1)): # run code below for every timestep in the ys batch\n",
        "        predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                      dec_hidden.to(device), \n",
        "                                      enc_output.to(device))\n",
        "        loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "        dec_input = torch.argmax(predictions, dim=1).unsqueeze(1)\n",
        "        curr_output[:, t] = dec_input.squeeze(1)\n",
        "    final_output[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE] = curr_output\n",
        "    target_output[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE] = targ\n",
        "    batch_loss = (loss / int(ys.size(1)))\n",
        "    total_loss += batch_loss\n",
        "\n",
        "print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                    total_loss / N_BATCH))\n",
        "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQKYT5w3n82V",
        "colab_type": "text"
      },
      "source": [
        "# Bleu Score Calculation for evaluation\n",
        "\n",
        "Read more about Bleu Score at :\n",
        "\n",
        "\n",
        "1.   https://en.wikipedia.org/wiki/BLEU\n",
        "2.   https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "\n",
        "We expect your BLEU Scores to be in the range of for full credit. No partial credit :( \n",
        "\n",
        "\n",
        "*   BLEU-1 > 0.14\n",
        "*   BLEU-2 > 0.08\n",
        "*   BLEU-3 > 0.02\n",
        "*   BLEU-4 > 0.15\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPosimvgdx_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_reference_candidate(target, pred):\n",
        "  reference = list(target)\n",
        "  reference = [targ_lang.idx2word[s] for s in np.array(reference[1:])]\n",
        "  candidate = list(pred)\n",
        "  candidate = [targ_lang.idx2word[s] for s in np.array(candidate[1:])]\n",
        "  return reference, candidate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6NzFQ16fZAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bleu_1 = 0.0\n",
        "bleu_2 = 0.0\n",
        "bleu_3 = 0.0\n",
        "bleu_4 = 0.0\n",
        "smoother = SmoothingFunction()\n",
        "save_reference = []\n",
        "\n",
        "for i in range(len(target_tensor_val)):\n",
        "  reference, candidate = get_reference_candidate(target_output[i], final_output[i])\n",
        "  #print(reference)\n",
        "  #print(candidate)\n",
        "  save_reference.append(reference)\n",
        "\n",
        "  bleu_1 += sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoother.method1)\n",
        "  bleu_2 += sentence_bleu(reference, candidate, weights=(0, 1, 0, 0), smoothing_function=smoother.method2)\n",
        "  bleu_3 += sentence_bleu(reference, candidate, weights=(0, 0, 1, 0), smoothing_function=smoother.method3)\n",
        "  bleu_4 += sentence_bleu(reference, candidate, weights=(0, 0, 0, 1), smoothing_function=smoother.method4)\n",
        "\n",
        "print('Individual 1-gram: %f' % (bleu_1/len(target_tensor_val)))\n",
        "print('Individual 2-gram: %f' % (bleu_2/len(target_tensor_val)))\n",
        "print('Individual 3-gram: %f' % (bleu_3/len(target_tensor_val)))\n",
        "print('Individual 4-gram: %f' % (bleu_4/len(target_tensor_val)))\n",
        "assert(len(save_reference)==len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYKSWRO93Mjz",
        "colab_type": "text"
      },
      "source": [
        "# Save File for Submission\n",
        "You just need to submit your **results.pickle** file to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Jnt32ItMA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_VdTMfb1M5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('../drive/My Drive/results.pickle', 'wb') as fil:\n",
        "    pickle.dump(save_candidate, fil)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}